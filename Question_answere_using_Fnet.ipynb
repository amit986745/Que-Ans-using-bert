{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a682ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5970510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85445ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8192\n",
    "MAX_SAMPLES = 50000\n",
    "BUFFER_SIZE = 20000\n",
    "MAX_LENGTH = 40\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "290c6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = r\"C:\\Users\\AMIT PAREEK\\Documents\\project deploment\\NLP\\archive (9)\\cornell movie-dialogs corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "edd080f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55066e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4ae8a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\AMIT PAREEK\\\\Documents\\\\project deploment\\\\NLP\\\\archive (9)\\\\cornell movie-dialogs corpus'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55b0e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a25f636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations():\n",
    "    # Helper function for loading the conversation splits\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        # get conversation in a list of line ID\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(id2line[conversation[i]])\n",
    "            outputs.append(id2line[conversation[i + 1]])\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "253c93b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b1ca1ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8c77cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((questions[:40000], answers[:40000]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((questions[40000:], answers[40000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce18af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
    "    # Replacing multiple continuous spaces with a single space\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
    "    # Replacing non english words with spaces\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "89cc12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = layers.TextVectorization(\n",
    "    VOCAB_SIZE,\n",
    "    standardize=preprocess_text,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cd8416f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9575ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(inputs, outputs):\n",
    "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
    "    # One extra padding token to the right to match the output shape\n",
    "    outputs = tf.pad(outputs, [[0, 1]])\n",
    "    return (\n",
    "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
    "        {\"outputs\": outputs[1:]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3444204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = train_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e87209bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_dataset.cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset = val_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fc78cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
    "        super(FNetEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Casting the inputs to complex64\n",
    "        inp_complex = tf.cast(inputs, tf.complex64)\n",
    "        # Projecting the inputs to the frequency domain using FFT2D and\n",
    "        # extracting the real part of the output\n",
    "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
    "        proj_input = self.layernorm_1(inputs + fft)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14a50fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"token_embeddings\": self.token_embeddings,\n",
    "            \"position_embeddings\": self.position_embeddings,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "86279b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(FNetDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b80155d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(\n",
    "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
    "    )\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model(\n",
    "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
    "    )\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
    "    return fnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c69ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "431fd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet = create_model()\n",
    "fnet.compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d234890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_6 (Positi  (None, None, 256)   2107392     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " f_net_encoder_3 (FNetEncoder)  (None, None, 256)    263936      ['positional_embedding_6[0][0]'] \n",
      "                                                                                                  \n",
      " outputs (Functional)           (None, None, 8192)   8684288     ['decoder_inputs[0][0]',         \n",
      "                                                                  'f_net_encoder_3[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,055,616\n",
      "Trainable params: 11,055,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26b99146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 963s 2s/step - loss: 1.6261 - accuracy: 0.2794 - val_loss: 1.4489 - val_accuracy: 0.3162\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1068s 2s/step - loss: 1.4618 - accuracy: 0.3124 - val_loss: 1.4140 - val_accuracy: 0.3253\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 986s 2s/step - loss: 1.4033 - accuracy: 0.3254 - val_loss: 1.3989 - val_accuracy: 0.3298\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 901s 1s/step - loss: 1.3623 - accuracy: 0.3348 - val_loss: 1.3938 - val_accuracy: 0.3333\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 936s 1s/step - loss: 1.3262 - accuracy: 0.3439 - val_loss: 1.3915 - val_accuracy: 0.3362\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1001s 2s/step - loss: 1.2927 - accuracy: 0.3521 - val_loss: 1.3994 - val_accuracy: 0.3320\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 978s 2s/step - loss: 1.2596 - accuracy: 0.3616 - val_loss: 1.4095 - val_accuracy: 0.3322\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 973s 2s/step - loss: 1.2244 - accuracy: 0.3727 - val_loss: 1.4235 - val_accuracy: 0.3294\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 981s 2s/step - loss: 1.1912 - accuracy: 0.3821 - val_loss: 1.4384 - val_accuracy: 0.3260\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 919s 1s/step - loss: 1.1561 - accuracy: 0.3936 - val_loss: 1.4595 - val_accuracy: 0.3250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x224594c91c0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnet.fit(train_dataset, epochs=10, batch_size=1000,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "287f2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faa875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c4c7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m not going to be a little . '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def decode_sentence(input_sentence):\n",
    "    # Mapping the input sentence to tokens and adding start and end tokens\n",
    "    tokenized_input_sentence = vectorizer(\n",
    "        tf.constant(\"[start] \" + preprocess_text(input_sentence) + \" [end]\")\n",
    "    )\n",
    "    # Initializing the initial sentence consisting of only the start token.\n",
    "    tokenized_target_sentence = tf.expand_dims(VOCAB.index(\"[start]\"), 0)\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # Get the predictions\n",
    "        predictions = fnet.predict(\n",
    "            {\n",
    "                \"encoder_inputs\": tf.expand_dims(tokenized_input_sentence, 0),\n",
    "                \"decoder_inputs\": tf.expand_dims(\n",
    "                    tf.pad(\n",
    "                        tokenized_target_sentence,\n",
    "                        [[0, MAX_LENGTH - tf.shape(tokenized_target_sentence)[0]]],\n",
    "                    ),\n",
    "                    0,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        # Calculating the token with maximum probability and getting the corresponding word\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :])\n",
    "        sampled_token = VOCAB[sampled_token_index.numpy()]\n",
    "        # If sampled token is the end token then stop generating and return the sentence\n",
    "        if tf.equal(sampled_token_index, VOCAB.index(\"[end]\")):\n",
    "            break\n",
    "        decoded_sentence += sampled_token + \" \"\n",
    "        tokenized_target_sentence = tf.concat(\n",
    "            [tokenized_target_sentence, [sampled_token_index]], 0\n",
    "        )\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c78a8f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'you re not a [UNK] . '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(\"Where have you been all this time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b4c802b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnet.save(\"fnet.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
